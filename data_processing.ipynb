{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af131a8",
   "metadata": {},
   "source": [
    "## 2. Segregates products by price/brand into â€œexpensiveâ€ vs.â€œaffordableâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646e92cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/cps45z1s63bfdjdypj31_9vc0000gn/T/ipykernel_52967/2408138780.py:294: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['price_tier'] = price_tiers\n",
      "/var/folders/k6/cps45z1s63bfdjdypj31_9vc0000gn/T/ipykernel_52967/2408138780.py:295: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['brand_tier'] = brand_tiers\n",
      "/var/folders/k6/cps45z1s63bfdjdypj31_9vc0000gn/T/ipykernel_52967/2408138780.py:296: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_tier'] = combined_tiers\n",
      "/var/folders/k6/cps45z1s63bfdjdypj31_9vc0000gn/T/ipykernel_52967/2408138780.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segregation'] = df['combined_tier']  # Final segregation column\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Segregation Complete!\n",
      "============================================================\n",
      "Output CSV: segregated_amazon_products.csv\n",
      "Workflow Diagram: segregation_workflow.png\n",
      "\n",
      "Segregation Distribution:\n",
      "segregation\n",
      "affordable    11147\n",
      "expensive      4337\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Output:\n",
      "                                                                                                                            product_name          brand category gender  price  discounted_price price_tier brand_tier combined_tier segregation  affiliate_link  product_url                                                          image_clean                                                  image_original                                                      image_url source_platform\n",
      "                                                                                                                          Bhains Ki Ankh Bhains Ki Ankh    Shoes  Women  699.0             299.0 affordable     budget    affordable  affordable             NaN          NaN                               bhains-ki-ankh-bhains-ki-ankh-nobg.jpg                                     bhains-ki-ankh-original.jpg https://m.media-amazon.com/images/I/5132STUkQCL._AC_UL320_.jpg    Amazon India\n",
      "                                 Extra Soft Fashionable & Stylish Slippers/High Heels Designer Wedges Sandal for Girls/Women (White, 37)       ArranQue    Shoes  Women  899.0             499.0 affordable     budget    affordable  affordable             NaN          NaN extra-soft-fashionable-stylish-slippershigh-heels--arranque-nobg.jpg extra-soft-fashionable-stylish-slippershigh-heels--original.jpg https://m.media-amazon.com/images/I/61z5qea+ofL._AC_UL320_.jpg    Amazon India\n",
      "Platform Strappy Heel Sandals Square Open Toe Two Strap Chunky High Heeled Sandals with Buckle Ankle Strap for Women Party Wedding Dress       JM LOOKS    Shoes  Women 1995.0             849.0 affordable     budget    affordable  affordable             NaN          NaN platform-strappy-heel-sandals-square-open-toe-two--jm-looks-nobg.jpg platform-strappy-heel-sandals-square-open-toe-two--original.jpg https://m.media-amazon.com/images/I/51XzC2oXeCL._AC_UL320_.jpg    Amazon India\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import networkx as nx\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "# ========== BRAND TIER DICTIONARIES ========== \n",
    "# (Abbreviated for space - use your full dictionaries here)\n",
    "LUXURY_BRANDS = {\n",
    "    \"Shoes\": {\n",
    "        \"Women\": [\"Chanel\", \"Christian Louboutin\", \"Gucci\", \"Jimmy Choo\", \"Manolo Blahnik\", \"Prada\", \"Saint Laurent\", \"Valentino\", \"Bottega Veneta\"],\n",
    "        \"Men\": [\"Berluti\", \"Brunello Cucinelli\", \"Church's\", \"John Lobb\", \"Alden\", \"Crockett & Jones\", \"Edward Green\", \"Gaziano & Girling\", \"Santoni\"],\n",
    "        \"Unisex\": [\"Acne Studios\", \"Balenciaga\", \"Common Projects\", \"Golden Goose\", \"Maison Margiela\", \"Rick Owens\", \"Veja\", \"Axel Arigato\"]\n",
    "    },\n",
    "    \"Bags\": {\n",
    "        \"Women\": [\"CÃ©line\", \"Chanel\", \"Dior\", \"Gucci\", \"HermÃ¨s\", \"Louis Vuitton\", \"Loewe\", \"Prada\", \"Saint Laurent\", \"Bottega Veneta\"],\n",
    "        \"Men\": [\"Berluti\", \"Dunhill\", \"Gucci\", \"HermÃ¨s\", \"Louis Vuitton\", \"Montblanc\", \"Prada\", \"Saint Laurent\", \"Goyard\"],\n",
    "        \"Unisex\": [\"Away\", \"Globe-Trotter\", \"Rimowa\", \"Monos\", \"BÃ©is\"]\n",
    "    },\n",
    "    \"Accessories\": {\n",
    "        \"Women\": [\"Chanel\", \"Dior\", \"Gucci\", \"HermÃ¨s\", \"Prada\", \"Saint Laurent\"],\n",
    "        \"Men\": [\"Brunello Cucinelli\", \"Dunhill\", \"HermÃ¨s\", \"Tom Ford\", \"Berluti\"],\n",
    "        \"Unisex\": [\"Apple\", \"Bang & Olufsen\", \"Bose\", \"Master & Dynamic\", \"Sony\"]\n",
    "    },\n",
    "    \"Clothing\": {\n",
    "        \"Women\": [\"Chanel\", \"Dior\", \"Gucci\", \"HermÃ¨s\", \"Prada\", \"Saint Laurent\"],\n",
    "        \"Men\": [\"Brioni\", \"Brunello Cucinelli\", \"HermÃ¨s\", \"Tom Ford\", \"Loro Piana\"],\n",
    "        \"Unisex\": [\"Acne Studios\", \"Fear of God Essentials\", \"Lemaire\", \"Stone Island\"]\n",
    "    },\n",
    "    \"Jewelry\": {\n",
    "        \"Women\": [\"Bulgari\", \"Cartier\", \"Tiffany & Co.\", \"Van Cleef & Arpels\"],\n",
    "        \"Men\": [\"Cartier\", \"David Yurman\", \"Tiffany & Co.\", \"John Hardy\"],\n",
    "        \"Unisex\": [\"Bulgari\", \"Cartier\", \"Tiffany & Co.\", \"Van Cleef & Arpels\"]\n",
    "    },\n",
    "    \"Watches\": {\n",
    "        \"Women\": [\"Cartier\", \"Chanel\", \"HermÃ¨s\", \"Patek Philippe\", \"Rolex\"],\n",
    "        \"Men\": [\"Audemars Piguet\", \"Patek Philippe\", \"Rolex\", \"Vacheron Constantin\"],\n",
    "        \"Unisex\": [\"Apple\", \"Cartier\", \"HermÃ¨s\", \"Omega\", \"Rolex\"]\n",
    "    },\n",
    "    \"Beauty\": {\n",
    "        \"Women\": [\"Chanel\", \"ClÃ© de Peau BeautÃ©\", \"La Mer\", \"Sisley\", \"SK-II\"],\n",
    "        \"Men\": [\"Chanel\", \"Dior Homme\", \"Gucci\", \"HermÃ¨s\", \"Tom Ford\"],\n",
    "        \"Unisex\": [\"Byredo\", \"Creed\", \"Le Labo\", \"Maison Margiela\", \"Tom Ford\"]\n",
    "    },\n",
    "    \"Home\": [\"Armani Casa\", \"HermÃ¨s Maison\", \"Ralph Lauren Home\", \"Versace Home\"],\n",
    "    \"Electronics\": [\"Apple\", \"Bang & Olufsen\", \"Master & Dynamic\", \"Sennheiser\"]\n",
    "}\n",
    "\n",
    "AFFORDABLE_BRANDS = {\n",
    "    \"Shoes\": {\n",
    "        \"Women\": [\"Adidas\", \"Allbirds\", \"Converse\", \"Everlane\", \"Nike\", \"Veja\", \"Rothy's\"],\n",
    "        \"Men\": [\"Adidas\", \"Allbirds\", \"Con Apple\", \"Converse\", \"Everlane\", \"Nike\"],\n",
    "        \"Unisex\": [\"Adidas\", \"Allbirds\", \"Converse\", \"Nike\", \"Veja\"]\n",
    "    },\n",
    "    \"Bags\": {\n",
    "        \"Women\": [\"Anthropologie\", \"Cuyana\", \"Everlane\", \"J.Crew\", \"Madewell\"],\n",
    "        \"Men\": [\"Everlane\", \"FjÃ¤llrÃ¤ven\", \"Herschel Supply Co.\", \"Patagonia\"],\n",
    "        \"Unisex\": [\"American Tourister\", \"Away\", \"BÃ©is\", \"Calpaks\", \"Monos\"]\n",
    "    },\n",
    "    \"Accessories\": {\n",
    "        \"Women\": [\"Coastal\", \"EyeBuyDirect\", \"Warby Parker\", \"Zenni Optical\"],\n",
    "        \"Men\": [\"Coastal\", \"EyeBuyDirect\", \"Warby Parker\", \"Zenni Optical\"],\n",
    "        \"Unisex\": [\"Anker\", \"Aukey\", \"Belkin\", \"Mophie\", \"RAVPower\"]\n",
    "    },\n",
    "    \"Clothing\": {\n",
    "        \"Women\": [\"& Other Stories\", \"COS\", \"H&M\", \"Uniqlo\", \"Zara\"],\n",
    "        \"Men\": [\"COS\", \"H&M\", \"Massimo Dutti\", \"Uniqlo\", \"Zara\"],\n",
    "        \"Unisex\": [\"Champion\", \"COS\", \"Monki\", \"Uniqlo\", \"Weekday\"]\n",
    "    },\n",
    "    \"Jewelry\": {\n",
    "        \"Women\": [\"Alex and Ani\", \"Kendra Scott\", \"Pandora\", \"Swarovski\"],\n",
    "        \"Men\": [\"Casio\", \"Fossil\", \"Pandora\", \"Swarovski\", \"Timex\"],\n",
    "        \"Unisex\": [\"Alex and Ani\", \"Fossil\", \"Pandora\", \"Swarovski\", \"Timex\"]\n",
    "    },\n",
    "    \"Watches\": {\n",
    "        \"Women\": [\"Casio\", \"Citizen\", \"Fossil\", \"Seiko\", \"Timex\"],\n",
    "        \"Men\": [\"Casio\", \"Citizen\", \"Fossil\", \"Seiko\", \"Timex\"],\n",
    "        \"Unisex\": [\"Casio\", \"Citizen\", \"Fossil\", \"Seiko\", \"Timex\"]\n",
    "    },\n",
    "    \"Beauty\": {\n",
    "        \"Women\": [\"CoverGirl\", \"L'OrÃ©al\", \"Maybelline\", \"Revlon\", \"Rimmel\"],\n",
    "        \"Men\": [\"Axe\", \"Gillette\", \"Nivea Men\", \"Old Spice\", \"Schick\"],\n",
    "        \"Unisex\": [\"Cerave\", \"Cetaphil\", \"Eucerin\", \"Nivea\", \"Vaseline\"]\n",
    "    },\n",
    "    \"Home\": [\"HomeGoods\", \"IKEA\", \"Target\", \"TJ Maxx\", \"Walmart\"],\n",
    "    \"Electronics\": [\"Audio-Technica\", \"Beats\", \"JBL\", \"Skullcandy\", \"Sony\"]\n",
    "}\n",
    "\n",
    "MID_TIER_BRANDS = {\n",
    "    \"Shoes\": {\n",
    "        \"Women\": [\"Coach\", \"Kate Spade\", \"Marc Jacobs\", \"Sam Edelman\", \"Tory Burch\", \"Skechers\"],\n",
    "        \"Men\": [\"Allen Edmonds\", \"Cole Haan\", \"Florsheim\", \"Johnston & Murphy\"],\n",
    "        \"Unisex\": [\"Allbirds\", \"Filling Pieces\", \"Greats\", \"Koio\", \"Veja\"]\n",
    "    },\n",
    "    \"Bags\": {\n",
    "        \"Women\": [\"Coach\", \"Kate Spade\", \"Marc Jacobs\", \"Michael Kors\", \"Tory Burch\"],\n",
    "        \"Men\": [\"Briggs & Riley\", \"Delsey\", \"Samsonite\", \"Travelpro\", \"Tumi\"],\n",
    "        \"Unisex\": [\"Briggs & Riley\", \"Delsey\", \"Samsonite\", \"Travelpro\", \"Tumi\"]\n",
    "    },\n",
    "    \"Accessories\": {\n",
    "        \"Women\": [\"Costa Del Mar\", \"Maui Jim\", \"Oakley\", \"Persol\", \"Ray-Ban\"],\n",
    "        \"Men\": [\"Costa Del Mar\", \"Maui Jim\", \"Oakley\", \"Persol\", \"Ray-Ban\"],\n",
    "        \"Unisex\": [\"Apple\", \"Fitbit\", \"Garmin\", \"Polar\", \"Samsung\"]\n",
    "    },\n",
    "    \"Clothing\": {\n",
    "        \"Women\": [\"Eileen Fisher\", \"Equipment\", \"Rag & Bone\", \"Theory\", \"Vince\"],\n",
    "        \"Men\": [\"7 For All Mankind\", \"Citizens of Humanity\", \"Rag & Bone\", \"Theory\", \"Vince\"],\n",
    "        \"Unisex\": [\"Arket\", \"COS\", \"Everlane\", \"Monki\", \"Weekday\"]\n",
    "    },\n",
    "    \"Jewelry\": {\n",
    "        \"Women\": [\"Alex and Ani\", \"Gorjana\", \"Kendra Scott\", \"Pandora\", \"Swarovski\"],\n",
    "        \"Men\": [\"David Yurman\", \"Fossil\", \"John Hardy\", \"Pandora\", \"Swarovski\"],\n",
    "        \"Unisex\": [\"Alex and Ani\", \"Catbird\", \"Mejuri\", \"Pandora\", \"Swarovski\"]\n",
    "    },\n",
    "    \"Watches\": {\n",
    "        \"Women\": [\"Fossil\", \"Kate Spade\", \"Marc Jacobs\", \"Michael Kors\", \"Tory Burch\"],\n",
    "        \"Men\": [\"Casio\", \"Citizen\", \"Fossil\", \"Seiko\", \"Timex\"],\n",
    "        \"Unisex\": [\"Apple\", \"Fitbit\", \"Garmin\", \"Polar\", \"Samsung\"]\n",
    "    },\n",
    "    \"Beauty\": {\n",
    "        \"Women\": [\"Benefit\", \"Clinique\", \"Tarte\", \"Too Faced\", \"Urban Decay\"],\n",
    "        \"Men\": [\"Baxter of California\", \"Clinique for Men\", \"Jack Black\", \"Kiehl's\"],\n",
    "        \"Unisex\": [\"Aveda\", \"Clinique\", \"Fresh\", \"Kiehl's\", \"Origins\"]\n",
    "    },\n",
    "    \"Home\": [\"CB2\", \"Crate & Barrel\", \"Pottery Barn\", \"West Elm\", \"Williams Sonoma\"],\n",
    "    \"Electronics\": [\"Beats\", \"Bose\", \"Harman Kardon\", \"JBL\", \"Sony\"]\n",
    "}\n",
    "\n",
    "BUDGET_BRANDS = {\n",
    "    \"Shoes\": {\n",
    "        \"Women\": [\"Bhains Ki Ankh\", \"ArranQue\", \"JM LOOKS\", \"SHOE CRAFT\", \"SilverArrow\", \"LUVFEET\", \"Theater\", \"XE Looks\", \"SELFIEE\", \"Get Glamr\", \"K KOMMY FASHIONS\", \"TRYME\", \"FASHIMO\", \"commander shoes\", \"Clouter hub\", \"Centrino\", \"DOCTOR EXTRA SOFT\", \"Campus\", \"LENSBURY\", \"Vendoz\", \"ASIAN\", \"Marc Loire\", \"AVANT\", \"STANPHORD\", \"AJANTA\", \"SPARX\", \"WELCOME\", \"Aqualite\", \"Boldfit\", \"AFROJACK\", \"Shoetopia\", \"Longwalk\", \"Delize\", \"LOVEHUSH\"],\n",
    "        \"Men\": [\"Bhains Ki Ankh\", \"ArranQue\", \"JM LOOKS\", \"SHOE CRAFT\", \"SilverArrow\", \"LUVFEET\", \"Theater\", \"XE Looks\", \"SELFIEE\", \"Get Glam\" , \"K KOMMY FASHIONS\", \"TRYME\", \"FASHIMO\", \"commander shoes\", \"Clouter hub\", \"Centrino\", \"DOCTOR EXTRA SOFT\", \"Campus\", \"LENSBURY\", \"Vendoz\", \"ASIAN\", \"Marc Loire\", \"AVANT\", \"STANPHORD\", \"AJANTA\", \"SPARX\", \"WELCOME\", \"Aqualite\", \"Boldfit\", \"AFROJACK\", \"Shoetopia\", \"Longwalk\", \"Delize\", \"LOVEHUSH\"],\n",
    "        \"Unisex\": [\"Bhains Ki Ankh\", \"ArranQue\", \"JM LOOKS\", \"SHOE CRAFT\", \"SilverArrow\", \"LUVFEET\", \"Theater\", \"XE Looks\", \"SELFIEE\", \"Get Glamr\", \"K KOMMY FASHIONS\", \"TRYME\", \"FASHIMO\", \"commander shoes\", \"Clouter hub\", \"Centrino\", \"DOCTOR EXTRA SOFT\", \"Campus\", \"LENSBURY\", \"Vendoz\", \"ASIAN\", \"Marc Loire\", \"AVANT\", \"STANPHORD\", \"AJANTA\", \"SPARX\", \"WELCOME\", \"Aqualite\", \"Boldfit\", \"AFROJACK\", \"Shoetopia\", \"Longwalk\", \"Delize\", \"LOVEHUSH\"]\n",
    "    },\n",
    "    \"Bags\": {\n",
    "        \"Women\": [],\n",
    "        \"Men\": [],\n",
    "        \"Unisex\": []\n",
    "    },\n",
    "    \"Accessories\": {\n",
    "        \"Women\": [],\n",
    "        \"Men\": [],\n",
    "        \"Unisex\": []\n",
    "    },\n",
    "    \"Clothing\": {\n",
    "        \"Women\": [\"AFROJACK\", \"ASIAN\", \"AVANT\", \"Boldfit\", \"Campus\", \"Centrino\", \"Clouter hub\", \"commander shoes\", \"Delize\", \"DOCTOR EXTRA SOFT\", \"FASHIMO\", \"Get Glamr\", \"K KOMMY FASHIONS\", \"LENSBURY\", \"Longwalk\", \"LOVEH-indentHUSH\", \"LUVFEET\", \"Marc Loire\", \"SELFIEE\", \"SHOE CRAFT\", \"Shoetopia\", \"SilverArrow\", \"STANPHORD\", \"Theater\", \"TRYME\", \"Vendoz\", \"XE Looks\"],\n",
    "        \"Men\": [\"AFROJACK\", \"ASIAN\", \"AVANT\", \"Boldfit\", \"Campus\", \"Centrino\", \"Clouter hub\", \"commander shoes\", \"Delize\", \"DOCTOR EXTRA SOFT\", \"FASHIMO\", \"Get Glamr\", \"K KOMMY FASHIONS\", \"LENSBURY\", \"Longwalk\", \"LOVEHUSH\", \"LUVFEET\", \"Marc Loire\", \"SELFIEE\", \"SHOE CRAFT\", \"Shoetopia\", \"SilverArrow\", \"STANPHORD\", \"Theater\", \"TRYME\", \"Vendoz\", \"XE Looks\"],\n",
    "        \"Unisex\": [\"AFROJACK\", \"ASIAN\", \"AVANT\", \"Boldfit\", \"Campus\", \"Centrino\", \"Clouter hub\", \"commander shoes\", \"Delize\", \"DOCTOR EXTRA SOFT\", \"FASHIMO\", \"Get Glamr\", \"K KOMMY FASHIONS\", \"LENSBURY\", \"Longwalk\", \"LOVEHUSH\", \"LUVFEET\", \"Marc Loire\", \"SELFIEE\", \"SHOE CRAFT\", \"Shoetopia\", \"SilverArrow\", \"STANPHORD\", \"Theater\", \"TRYME\", \"Vendoz\", \"XE Looks\"]\n",
    "    },\n",
    "    \"Jewelry\": {\n",
    "        \"Women\": [],\n",
    "        \"Men\": [],\n",
    "        \"Unisex\": []\n",
    "    },\n",
    "    \"Watches\": {\n",
    "        \"Women\": [\"AJANTA\", \"SPARX\"],\n",
    "        \"Men\": [\"AJANTA\", \"SPARX\"],\n",
    "        \"Unisex\": [\"AJANTA\", \"SPARX\"]\n",
    "    },\n",
    "    \"Beauty\": {\n",
    "        \"Women\": [\"Aqualite\"],\n",
    "        \"Men\": [],\n",
    "        \"Unisex\": []\n",
    "    },\n",
    "    \"Home\": [],\n",
    "    \"Electronics\": []\n",
    "}\n",
    "\n",
    "# ========== NORMALIZATION FUNCTIONS ==========\n",
    "def normalize_brand_dict(d):\n",
    "    normalized = {}\n",
    "    for category, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            normalized[category] = {}\n",
    "            for gender, brands in value.items():\n",
    "                normalized[category][gender] = [str(b).strip().lower() for b in brands]\n",
    "        else:\n",
    "            normalized[category] = [str(b).strip().lower() for b in value]\n",
    "    return normalized\n",
    "\n",
    "# Create normalized dictionaries\n",
    "norm_luxury = normalize_brand_dict(LUXURY_BRANDS)\n",
    "norm_affordable = normalize_brand_dict(AFFORDABLE_BRANDS)\n",
    "norm_mid_tier = normalize_brand_dict(MID_TIER_BRANDS)\n",
    "norm_budget = normalize_brand_dict(BUDGET_BRANDS)\n",
    "\n",
    "# ========== CLASSIFICATION FUNCTIONS ==========\n",
    "def get_brand_tier(brand, category, gender):\n",
    "    if pd.isna(brand) or not str(brand).strip():\n",
    "        return 'unknown'\n",
    "    \n",
    "    brand_norm = str(brand).strip().lower()\n",
    "    category = str(category).strip()\n",
    "    gender = str(gender).strip()\n",
    "    \n",
    "    # Non-gendered categories (Home/Electronics)\n",
    "    if category in ['Home', 'Electronics']:\n",
    "        if category in norm_luxury and brand_norm in norm_luxury[category]:\n",
    "            return 'luxury'\n",
    "        if category in norm_affordable and brand_norm in norm_affordable[category]:\n",
    "            return 'affordable'\n",
    "        if category in norm_mid_tier and brand_norm in norm_mid_tier[category]:\n",
    "            return 'mid-tier'\n",
    "        if category in norm_budget and brand_norm in norm_budget[category]:\n",
    "            return 'budget'\n",
    "        return 'unknown'\n",
    "    \n",
    "    # Gendered categories\n",
    "    if category in norm_luxury and gender in norm_luxury[category]:\n",
    "        if brand_norm in norm_luxury[category][gender]:\n",
    "            return 'luxury'\n",
    "    if category in norm_affordable and gender in norm_affordable[category]:\n",
    "        if brand_norm in norm_affordable[category][gender]:\n",
    "            return 'affordable'\n",
    "    if category in norm_mid_tier and gender in norm_mid_tier[category]:\n",
    "        if brand_norm in norm_mid_tier[category][gender]:\n",
    "            return 'mid-tier'\n",
    "    if category in norm_budget and gender in norm_budget[category]:\n",
    "        if brand_norm in norm_budget[category][gender]:\n",
    "            return 'budget'\n",
    "    return 'unknown'\n",
    "\n",
    "def classify_products(df):\n",
    "    # Clean and convert prices\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df = df.dropna(subset=['price'])\n",
    "    \n",
    "    # Calculate global percentiles\n",
    "    all_prices = df['price'].dropna().sort_values()\n",
    "    n = len(all_prices)\n",
    "    global_p25 = all_prices.iloc[math.floor(n * 0.25)] if n > 0 else 0\n",
    "    global_p75 = all_prices.iloc[math.ceil(n * 0.75) - 1] if n > 0 else 0\n",
    "    \n",
    "    # Group by category and gender\n",
    "    grouped = df.groupby(['category', 'gender'])\n",
    "    group_stats = {}\n",
    "    \n",
    "    for key, group in grouped:\n",
    "        prices = group['price'].dropna().sort_values()\n",
    "        n_group = len(prices)\n",
    "        if n_group >= 5:\n",
    "            p25 = prices.iloc[math.floor(n_group * 0.25)]\n",
    "            p75 = prices.iloc[math.ceil(n_group * 0.75) - 1]\n",
    "        else:\n",
    "            p25 = global_p25\n",
    "            p75 = global_p75\n",
    "        group_stats[key] = (p25, p75)\n",
    "    \n",
    "    # Classify each product\n",
    "    price_tiers = []\n",
    "    brand_tiers = []\n",
    "    combined_tiers = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Get group stats\n",
    "        key = (row['category'], row['gender'])\n",
    "        p25, p75 = group_stats.get(key, (global_p25, global_p75))\n",
    "        price = row['price']\n",
    "        \n",
    "        # Price tier classification\n",
    "        if price >= 10000 or price >= p75:\n",
    "            price_tier = 'expensive'\n",
    "        elif price <= 4000 or price <= p25:\n",
    "            price_tier = 'affordable'\n",
    "        else:\n",
    "            price_tier = 'mid-range'\n",
    "        \n",
    "        # Brand tier classification\n",
    "        brand_tier = get_brand_tier(\n",
    "            row['brand'], \n",
    "            row['category'], \n",
    "            row['gender']\n",
    "        )\n",
    "        \n",
    "        # Combined classification\n",
    "        if brand_tier == 'luxury':\n",
    "            combined_tier = 'expensive'\n",
    "        elif brand_tier in ['affordable', 'budget']:\n",
    "            combined_tier = 'affordable'\n",
    "        else:  # mid-tier or unknown\n",
    "            combined_tier = 'affordable' if price_tier == 'affordable' else 'expensive'\n",
    "        \n",
    "        price_tiers.append(price_tier)\n",
    "        brand_tiers.append(brand_tier)\n",
    "        combined_tiers.append(combined_tier)\n",
    "    \n",
    "    # Add new columns\n",
    "    df['price_tier'] = price_tiers\n",
    "    df['brand_tier'] = brand_tiers\n",
    "    df['combined_tier'] = combined_tiers\n",
    "    df['segregation'] = df['combined_tier']  # Final segregation column\n",
    "    \n",
    "    return df, global_p25, global_p75\n",
    "\n",
    "# ========== VISUALIZATION FUNCTIONS ==========\n",
    "def create_workflow_diagram(global_p25, global_p75):\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create nodes\n",
    "    nodes = {\n",
    "        'load': (1, 9, 'Load Data', '#4CAF50'),\n",
    "        'clean': (3, 9, 'Clean Prices', '#FFC107'),\n",
    "        'global': (5, 9, 'Global Stats\\np25={:.0f}\\np75={:.0f}'.format(global_p25, global_p75), '#2196F3'),\n",
    "        'group': (3, 7, 'Group by\\nCategory+Gender', '#9C27B0'),\n",
    "        'price': (1, 5, 'Price Tier\\n(â‰¥â‚¹10k/â‰¥p75=Expensive\\nâ‰¤â‚¹4k/â‰¤p25=Affordable)', '#F44336'),\n",
    "        'brand': (5, 5, 'Brand Tier\\n(Luxury/Affordable/Mid/Budget)', '#FF9800'),\n",
    "        'combine': (3, 3, 'Combine Classifications', '#00BCD4'),\n",
    "        'save': (3, 1, 'Save Structured CSV', '#8BC34A')\n",
    "    }\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node, (x, y, label, color) in nodes.items():\n",
    "        ax.add_patch(plt.Circle((x, y), 0.5, color=color, alpha=0.8))\n",
    "        plt.text(x, y, label, ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Draw connections\n",
    "    connections = [\n",
    "        ('load', 'clean'), ('clean', 'global'), ('clean', 'group'),\n",
    "        ('global', 'group'), ('group', 'price'), ('group', 'brand'),\n",
    "        ('price', 'combine'), ('brand', 'combine'), ('combine', 'save')\n",
    "    ]\n",
    "    \n",
    "    for start, end in connections:\n",
    "        x1, y1, _, _ = nodes[start]\n",
    "        x2, y2, _, _ = nodes[end]\n",
    "        plt.plot([x1, x2], [y1, y2], 'k-', lw=2, alpha=0.6)\n",
    "    \n",
    "    # Add title\n",
    "    plt.title('Product Segregation Workflow', fontsize=16, pad=20)\n",
    "    \n",
    "    # Save diagram\n",
    "    plt.savefig('segregation_workflow.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'segregation_workflow.png'\n",
    "\n",
    "# ========== MAIN EXECUTION ==========\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and process data\n",
    "    df = pd.read_csv('Output/amazon_products.csv')\n",
    "    classified_df, global_p25, global_p75 = classify_products(df)\n",
    "    \n",
    "    # Format and save results\n",
    "    output_df = classified_df[[\n",
    "        'product_name', 'brand', 'category', 'gender', 'price', \n",
    "        'discounted_price', 'price_tier', 'brand_tier', 'combined_tier', 'segregation',\n",
    "        'affiliate_link', 'product_url', 'image_clean', 'image_original',\n",
    "        'image_url', 'source_platform'\n",
    "    ]]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_df.to_csv('segregated_amazon_products.csv', index=False)\n",
    "    \n",
    "    # Create and save workflow diagram\n",
    "    diagram_path = create_workflow_diagram(global_p25, global_p75)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Segregation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Output CSV: segregated_amazon_products.csv\")\n",
    "    print(f\"Workflow Diagram: {diagram_path}\")\n",
    "    print(\"\\nSegregation Distribution:\")\n",
    "    print(output_df['segregation'].value_counts())\n",
    "    print(\"\\nSample Output:\")\n",
    "    print(output_df.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708e90c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns dropped and file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your CSV file\n",
    "file_path = \"Output/segregated_amazon_products.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Load the CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Drop specified columns if they exist\n",
    "    columns_to_drop = ['image_clean', 'affiliate_link']\n",
    "    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "    # Save the modified CSV back to the same location\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "    print(\"Columns dropped and file updated successfully.\")\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaa570",
   "metadata": {},
   "source": [
    "## Image Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93d686d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 15484 products from Output/segregated_amazon_products.csv\n",
      "Image not found: amazon_scrape_20250703_190143/images/tru-hair-skin-hair-wax-cream-for-men-100g-12hour-s-original.jpg\n",
      "Image not found: amazon_scrape_20250703_190143/images/sifa-carpet-kashmiri-silk-traditional-design-recta-original.jpg\n",
      "âœ… Computed embeddings for 15482 images\n",
      "âœ… Saved local embeddings to 'Output/local_product_embeddings.npy' and '.csv'\n",
      "â„¹ï¸ Collection product_embeddings already exists\n",
      "âœ… Batch 0 uploaded successfully\n",
      "âœ… Batch 1 uploaded successfully\n",
      "âœ… Batch 4 uploaded successfully\n",
      "âš ï¸ Retry 1 for batch 2 failed: The write operation timed out\n",
      "âš ï¸ Retry 1 for batch 3 failed: The write operation timed out\n",
      "âš ï¸ Retry 1 for batch 5 failed: The write operation timed out\n",
      "âœ… Batch 6 uploaded successfully\n",
      "âœ… Batch 8 uploaded successfully\n",
      "âœ… Batch 7 uploaded successfully\n",
      "âœ… Batch 10 uploaded successfully\n",
      "âœ… Batch 9 uploaded successfully\n",
      "âš ï¸ Retry 2 for batch 3 failed: The write operation timed out\n",
      "âš ï¸ Retry 2 for batch 2 failed: The write operation timed out\n",
      "âš ï¸ Retry 2 for batch 5 failed: The write operation timed out\n",
      "âœ… Batch 11 uploaded successfully\n",
      "âš ï¸ Retry 1 for batch 12 failed: The write operation timed out\n",
      "âœ… Batch 13 uploaded successfully\n",
      "âœ… Batch 14 uploaded successfully\n",
      "âœ… Batch 15 uploaded successfully\n",
      "âœ… Batch 12 uploaded successfully\n",
      "âœ… Batch 2 uploaded successfully\n",
      "âš ï¸ Retry 3 for batch 3 failed: The write operation timed out\n",
      "âš ï¸ Retry 3 for batch 5 failed: The write operation timed out\n",
      "âš ï¸ Retry 1 for batch 16 failed: The write operation timed out\n",
      "âœ… Batch 18 uploaded successfully\n",
      "âœ… Batch 17 uploaded successfully\n",
      "âœ… Batch 19 uploaded successfully\n",
      "âœ… Batch 20 uploaded successfully\n",
      "âœ… Batch 21 uploaded successfully\n",
      "âœ… Batch 22 uploaded successfully\n",
      "âŒ Batch 3 failed after 3 attempts\n",
      "âŒ Batch 5 failed after 3 attempts\n",
      "âš ï¸ Retry 2 for batch 16 failed: The write operation timed out\n",
      "âœ… Batch 24 uploaded successfully\n",
      "âš ï¸ Retry 1 for batch 23 failed: The write operation timed out\n",
      "âœ… Batch 27 uploaded successfully\n",
      "âœ… Batch 25 uploaded successfully\n",
      "âœ… Batch 26 uploaded successfully\n",
      "âœ… Batch 28 uploaded successfully\n",
      "âœ… Batch 29 uploaded successfully\n",
      "âœ… Batch 30 uploaded successfully\n",
      "âœ… Batch 16 uploaded successfully\n",
      "âœ… Batch 23 uploaded successfully\n",
      "\n",
      "âœ… Completed: 29 batches\n",
      "âŒ Failed: 2 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/cps45z1s63bfdjdypj31_9vc0000gn/T/ipykernel_96580/1689270235.py:138: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_result = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Price Tier: affordable\n",
      "  ðŸ›ï¸ Product: | Casual Elegance: Mid-Waist Printed Pleated Midi Skirt for Women\n",
      "     ðŸ”— Similarity: 1.0000\n",
      "     ðŸ’° Price: 1999.0 (Discounted: 399.0)\n",
      "  ðŸ›ï¸ Product: | Casual Long Skirt, Regular Fit Maxi Skirt with High Waist Print and Pleated Style, Women's Long Skirt\n",
      "     ðŸ”— Similarity: 0.9185\n",
      "     ðŸ’° Price: 1999.0 (Discounted: 299.0)\n",
      "  ðŸ›ï¸ Product: | Stylish Printed Skirt | Embrace Effortless Elegance with The Women | Floral Print Pleated Maxi Skirt Skirt\n",
      "     ðŸ”— Similarity: 0.8855\n",
      "     ðŸ’° Price: 1999.0 (Discounted: 299.0)\n",
      "\n",
      "ðŸ”¹ Price Tier: expensive\n",
      "  ðŸ›ï¸ Product: Cotton Maternity Dresses for Women with Zip || Feeding Kurtis for Women || Feeding Dresses for Nursing || Stylish Maternity Wear.\n",
      "     ðŸ”— Similarity: 0.8320\n",
      "     ðŸ’° Price: 2999.0 (Discounted: 499.0)\n",
      "  ðŸ›ï¸ Product: Cotton Maternity Dresses for Women with Zip || Feeding Kurtis for Women || Feeding Dresses for Nursing || Stylish Maternity Wear On Latest Prints.\n",
      "     ðŸ”— Similarity: 0.8320\n",
      "     ðŸ’° Price: 2999.0 (Discounted: 499.0)\n",
      "  ðŸ›ï¸ Product: Women's Floral Printed Polyester Backless Cami Dress | Beach Dresses Swimwear | Semi Sheer Beach wear Cover-Up Dress for Women & Girls Teen\n",
      "     ðŸ”— Similarity: 0.8275\n",
      "     ðŸ’° Price: 2599.0 (Discounted: 1199.0)\n",
      "\n",
      "ðŸ”¹ Price Tier: mid-range\n",
      "  ðŸ›ï¸ Product: Carlton London\n",
      "     ðŸ”— Similarity: 0.7263\n",
      "     ðŸ’° Price: 5490.0 (Discounted: 3843.0)\n",
      "  ðŸ›ï¸ Product: Carlton London\n",
      "     ðŸ”— Similarity: 0.7263\n",
      "     ðŸ’° Price: 5400.0 (Discounted: 3236.0)\n",
      "  ðŸ›ï¸ Product: Carlton London\n",
      "     ðŸ”— Similarity: 0.7263\n",
      "     ðŸ’° Price: 5390.0 (Discounted: 3770.0)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import clip\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load product metadata CSV\n",
    "csv_path = \"Output/segregated_amazon_products.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded {len(df)} products from {csv_path}\")\n",
    "\n",
    "# Prepare image embeddings\n",
    "image_folder = \"amazon_scrape_20250703_190143/images\"\n",
    "embeddings = []\n",
    "metadata = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_file = row.get(\"image_original\", None)\n",
    "    if pd.isna(image_file):\n",
    "        continue\n",
    "\n",
    "    image_path = os.path.join(image_folder, image_file)\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = model.encode_image(image_tensor).cpu().numpy()\n",
    "        embedding /= np.linalg.norm(embedding, axis=1, keepdims=True)\n",
    "\n",
    "        embeddings.append(embedding.flatten())\n",
    "        metadata.append(row.to_dict())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "print(f\"âœ… Computed embeddings for {len(embeddings)} images\")\n",
    "\n",
    "# Save embeddings locally\n",
    "embedding_array = np.array(embeddings)\n",
    "os.makedirs(\"Output\", exist_ok=True)\n",
    "np.save(\"Output/local_product_embeddings.npy\", embedding_array)\n",
    "np.savetxt(\"Output/local_product_embeddings.csv\", embedding_array, delimiter=\",\")\n",
    "print(\"âœ… Saved local embeddings to 'Output/local_product_embeddings.npy' and '.csv'\")\n",
    "\n",
    "# Initialize Qdrant\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://06d82058-ef9d-4c8b-8b18-58f5d5b87287.eu-central-1-0.aws.cloud.qdrant.io\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.r3UI0Ka4GvZic1fTZAwUzu1fg-fFxzDUakWXVimn5ss\"\n",
    ")\n",
    "collection_name = \"product_embeddings\"\n",
    "\n",
    "# Create Qdrant collection if it doesn't exist\n",
    "existing = qdrant_client.get_collections().collections\n",
    "if collection_name not in [col.name for col in existing]:\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=rest.VectorParams(size=512, distance=rest.Distance.COSINE),\n",
    "    )\n",
    "    print(f\"âœ… Created collection: {collection_name}\")\n",
    "else:\n",
    "    print(f\"â„¹ï¸ Collection {collection_name} already exists\")\n",
    "\n",
    "# Define batched + parallel Qdrant upsert logic\n",
    "def upload_batch(batch_id, batch_points, max_retries=3):\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            qdrant_client.upsert(collection_name=collection_name, points=batch_points)\n",
    "            print(f\"âœ… Batch {batch_id} uploaded successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Retry {attempt} for batch {batch_id} failed: {e}\")\n",
    "            time.sleep(2 * attempt)\n",
    "    print(f\"âŒ Batch {batch_id} failed after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "def create_batches(embeddings, metadata, batch_size=500):\n",
    "    total = len(embeddings)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_points = [\n",
    "            rest.PointStruct(id=i, vector=vec.tolist(), payload=meta)\n",
    "            for i, (vec, meta) in enumerate(zip(embeddings[start:end], metadata[start:end]), start)\n",
    "        ]\n",
    "        yield (start // batch_size, batch_points)\n",
    "\n",
    "def parallel_upsert(embeddings, metadata, batch_size=500, max_workers=6):\n",
    "    tasks = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for batch_id, batch_points in create_batches(embeddings, metadata, batch_size):\n",
    "            task = executor.submit(upload_batch, batch_id, batch_points)\n",
    "            tasks.append(task)\n",
    "\n",
    "        completed, failed = 0, 0\n",
    "        for future in as_completed(tasks):\n",
    "            if future.result():\n",
    "                completed += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "\n",
    "    print(f\"\\nâœ… Completed: {completed} batches\")\n",
    "    print(f\"âŒ Failed: {failed} batches\")\n",
    "\n",
    "# Upload to Qdrant in parallel\n",
    "if embeddings and metadata:\n",
    "    parallel_upsert(embeddings, metadata)\n",
    "else:\n",
    "    print(\"âš ï¸ No valid embeddings to upload to Qdrant.\")\n",
    "\n",
    "# Similarity Search Function\n",
    "def find_similar_images(query_image_path, top_k=5):\n",
    "    if not os.path.exists(query_image_path):\n",
    "        print(f\"âŒ Query image not found: {query_image_path}\")\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        query_image = preprocess(Image.open(query_image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = model.encode_image(query_image).cpu().numpy()\n",
    "        query_embedding /= np.linalg.norm(query_embedding)\n",
    "        query_vector = query_embedding.flatten().tolist()\n",
    "\n",
    "        search_result = qdrant_client.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=query_vector,\n",
    "            limit=1000,\n",
    "            with_payload=True\n",
    "        )\n",
    "\n",
    "        results = [(hit.payload, hit.score) for hit in search_result]\n",
    "\n",
    "        grouped = defaultdict(list)\n",
    "        for res, score in results:\n",
    "            tier = res.get(\"price_tier\", \"Unknown\")\n",
    "            grouped[tier].append((res, score))\n",
    "\n",
    "        for tier in grouped:\n",
    "            grouped[tier] = sorted(grouped[tier], key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "        return grouped\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in similarity search: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Run a test query\n",
    "query_image = \"amazon_scrape_20250703_190143/images/-casual-elegance-midwaist-printed-pleated-midi-ski-original.jpg\"\n",
    "results = find_similar_images(query_image, top_k=3)\n",
    "\n",
    "# Display Results\n",
    "for price_tier, products in results.items():\n",
    "    print(f\"\\nðŸ”¹ Price Tier: {price_tier}\")\n",
    "    for product, sim in products:\n",
    "        print(f\"  ðŸ›ï¸ Product: {product.get('product_name', 'N/A')}\")\n",
    "        print(f\"     ðŸ”— Similarity: {sim:.4f}\")\n",
    "        print(f\"     ðŸ’° Price: {product.get('price', 'N/A')} (Discounted: {product.get('discounted_price', 'N/A')})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da100a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47c24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Loading embeddings and metadata...\n",
      "ðŸ” Retrying batches: [3, 5]\n",
      "âœ… Batch 3 re-uploaded successfully\n",
      "âœ… Batch 5 re-uploaded successfully\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "collection_name = \"product_embeddings\"\n",
    "failed_batch_ids = [3, 5]  # Update this list with failed batch IDs (from your earlier output)\n",
    "batch_size = 500\n",
    "embedding_path = \"Output/local_product_embeddings.npy\"\n",
    "csv_path = \"Output/segregated_amazon_products.csv\"\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"ðŸ“¦ Loading embeddings and metadata...\")\n",
    "embeddings = np.load(embedding_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "metadata = df.to_dict(orient=\"records\")\n",
    "\n",
    "# === QDRANT SETUP ===\n",
    "qdrant_client = QdrantClient(\n",
    "    url=\"https://06d82058-ef9d-4c8b-8b18-58f5d5b87287.eu-central-1-0.aws.cloud.qdrant.io\",\n",
    "    api_key=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.r3UI0Ka4GvZic1fTZAwUzu1fg-fFxzDUakWXVimn5ss\"\n",
    ")\n",
    "\n",
    "# === RETRY LOGIC ===\n",
    "def upload_batch(batch_id, max_retries=3):\n",
    "    start = batch_id * batch_size\n",
    "    end = start + batch_size\n",
    "    batch_embeddings = embeddings[start:end]\n",
    "    batch_metadata = metadata[start:end]\n",
    "\n",
    "    batch_points = [\n",
    "        rest.PointStruct(id=i, vector=vec.tolist(), payload=meta)\n",
    "        for i, (vec, meta) in enumerate(zip(batch_embeddings, batch_metadata), start)\n",
    "    ]\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            qdrant_client.upsert(collection_name=collection_name, points=batch_points)\n",
    "            print(f\"âœ… Batch {batch_id} re-uploaded successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Retry {attempt} for batch {batch_id} failed: {e}\")\n",
    "            time.sleep(2 * attempt)\n",
    "    print(f\"âŒ Batch {batch_id} failed after {max_retries} retries\")\n",
    "    return False\n",
    "\n",
    "# === EXECUTE RETRIES ===\n",
    "print(f\"ðŸ” Retrying batches: {failed_batch_ids}\")\n",
    "for batch_id in failed_batch_ids:\n",
    "    upload_batch(batch_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d7520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
